{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quERsqITkjDm"
      },
      "source": [
        "# Image Captioning Computer Vision.\n",
        "\n",
        "Name: Anthony Kevin Oktavius\n",
        "\n",
        "Paper and Dataset can be found at this link: https://paperswithcode.com/dataset/flickrstyle10k"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJcCS8pN77Gw"
      },
      "source": [
        "Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SRJ0gzzqg0y0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import zipfile\n",
        "import gc\n",
        "import pickle\n",
        "import cv2\n",
        "import math\n",
        "import warnings\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import albumentations as A\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data_utils\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "# import torchmetrics\n",
        "import torch.nn.functional as F\n",
        "import PIL\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "# import pytorch_lightning as pl\n",
        "import imutils\n",
        "import zipfile\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.python.client import device_lib\n",
        "from zipfile import ZipFile\n",
        "from IPython import display\n",
        "from torchvision import models, transforms\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.metrics import confusion_matrix, roc_curve\n",
        "# from pytorch_lightning.loggers import TensorBoardLogger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynlGAMyT1yG7"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNF39Ee_wW_g"
      },
      "source": [
        "Download and Unzip the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VawoC7e3tXTE",
        "outputId": "96bbd6b8-2780-4f6d-c6dd-c20bed7dbd62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace ./Dataset/Images/1000268201_693b08cb0e.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ],
      "source": [
        "! cp ./kaggle.json ~/.kaggle/\n",
        "! kaggle datasets download -d adityajn105/flickr8k\n",
        "! unzip -qq ./flickr8k.zip -d ./Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9sDYH8i1wzg"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8igKi2iBzFvM"
      },
      "source": [
        "Create utility function to load datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c3DiPwn9BVGp"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.CenterCrop((320, 320)),\n",
        "    transforms.Resize((128, 128)), # Mitigate if there's a \"RAM is not enough\" error.\n",
        "    transforms.RandomRotation(degrees = 5),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(\n",
        "    #     mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n",
        "    # ),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.CenterCrop((224, 224)),\n",
        "    transforms.Resize((128, 128)), # Mitigate if there's a \"RAM is not enough\" error.\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(\n",
        "    #     mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],\n",
        "    # ),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess text"
      ],
      "metadata": {
        "id": "6yOOXjMmubZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "captions = open(\"Dataset/captions.txt\", 'r')\n",
        "\n",
        "image_name_to_labels_dict = defaultdict(list)\n",
        "for text_per_line in captions.readlines()[1:]:\n",
        "  image_name, label = text_per_line.split(\".jpg,\")\n",
        "  image_name = f\"{image_name}.jpg\"\n",
        "  label = label.strip(\"\\n\")\n",
        "\n",
        "  image_name_to_labels_dict[image_name].append(label)\n",
        "\n",
        "print(\"Example image_name to label dict: \")\n",
        "print(image_name_to_labels_dict[\"1000268201_693b08cb0e.jpg\"])\n",
        "captions.close()"
      ],
      "metadata": {
        "id": "xrTXHaoHD0Tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580be2dc-3ef9-4e9d-e4d6-fc2ee00fbd8c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example image_name to label dict: \n",
            "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-CAuM-zUwQN"
      },
      "source": [
        "Divide into train and test folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "cVPOcmH8tXq0",
        "outputId": "72055b1a-55fa-4d88-d74d-c7f901018da5"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndentationError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mast_parse\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mArguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mexactly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0mlibrary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         and are passed to the built-in compile function.\"\"\"\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mPyCF_ONLY_AST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_compiler_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndentationError\u001b[0m: expected an indented block after function definition on line 2 (<ipython-input-5-fef70f523d9c>, line 4)"
          ]
        }
      ],
      "source": [
        "class FlickrStyleDataset(Dataset):\n",
        "  def __init__(self, dataset_folder, dataset_labels, number_of_instances_per_class):\n",
        "    super(FlickrStyleDataset, self).__init__()\n",
        "    self.dataset_folder = dataset_folder\n",
        "    self.dataset_labels = dataset_labels\n",
        "    self.number_of_instances_per_class = number_of_instances_per_class\n",
        "\n",
        "  def __getitem__(self):\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S2yeRf8Uvu9",
        "outputId": "d3da89df-05f0-4912-ec46-d9f5677b585d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2513260012_03d33305cf.jpg\n",
            "7000\n",
            "7000\n"
          ]
        }
      ],
      "source": [
        "TEST_SIZE = 0.2\n",
        "TRAIN_SIZE = 1 - TEST_SIZE\n",
        "\n",
        "random_split = torch.utils.data.random_split([tensor_tmnist_dataset,])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDEZMNXgRXcb"
      },
      "source": [
        "Generate a dataset loader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFIwTbonFBI3"
      },
      "source": [
        "## Data Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbyvbJtt1wox"
      },
      "outputs": [],
      "source": [
        "class Seq2SeqImageCaptioningModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    self.\n",
        "\n",
        "  def forward():\n",
        "    pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}